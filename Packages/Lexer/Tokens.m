(* ::Package:: *)

(* Autogenerated Package *)

(* ::Section:: *)
(*Tokens*)



TokenStream::usage="An object representing a stream of tokens";


SetTokenizerCheckpoint::usage="Sets the Tokenizer checkpoint";
ResetTokenizerCheckpoint::usage="Reverts the Tokenizer checkpoint";
WithTokenizerCheckpoint::usage="Wrapper for tokenizing";


TokenRead::usage="Pulls n tokens out of a TokenStream";
TokenStreamer::usage="Lower-level object for faster token streaming";


Begin["`Private`"];


(* ::Subsection:: *)
(*TokenStream*)



RegisterInterface[
  TokenStream,
  {
    "Tokens",
    "Stream"
    },
  "Constructor"->buildTokenStream
  ]


(* ::Subsubsubsection::Closed:: *)
(*buildTokenStream*)



buildTokenStream[l_LexerObject, i_InputStream]:=
  <|"Tokens"->l["Tokens"], "Stream"->i|>;
buildTokenStream[l_LexerObject, s_String?(Not@*FileExistsQ)]:=
  buildTokenStream[l, StringToStream[s]];
buildTokenStream[l_LexerObject, s_String?(FileExistsQ)]:=
 buildTokenStream[l, OpenRead[s]];


(* ::Subsubsubsection::Closed:: *)
(*Methods*)



InterfaceMethod[TokenStream]@
  t_TokenStream["Read"][n:_Integer?Positive:1]:=
    TokenRead[t, n];
InterfaceMethod[TokenStream]@
  t_TokenStream["Streamer"][]:=
    TokenStreamer[t];
InterfaceMethod[TokenStream]@
  t_TokenStream["Close"][]:=
    Close@t["Stream"];


(* ::Subsection:: *)
(*TokenRead*)



(* ::Subsubsection::Closed:: *)
(*$checkpoints*)



(* ::Text:: *)
(*
	Might be better to do with Language`ExpressionStore ?
	We\[CloseCurlyQuote]re gonna manage this under the assumption that if you peek ahead and then reset you want to be able to jump back to where you were post peek...?
*)



If[!AssociationQ[$checkpoints], $checkpoints=<||>];
If[!AssociationQ[$peekpoints], $peekpoints=<||>];


SetTokenizerCheckpoint[stream_InputStream, dir:"Forward"|"Backward":"Backward"]:=
  If[dir==="Backward",
    $checkpoints[stream] = StreamPosition[stream],
    $peekpoints[stream] = StreamPosition[stream]
    ];
SetTokenizerCheckpoint[
  TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  SetTokenizerCheckpoint[stream, dir];
SetTokenizerCheckpoint[
  t_TokenStream, 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  SetTokenizerCheckpoint[t["Stream"], dir];


ResetTokenizerCheckpoint[stream_InputStream, dir:"Forward"|"Backward":"Backward"]:=
  SetStreamPosition[stream, 
    If[dir==="Backward",
      $checkpoints,
      $peekpoints
      ][stream]
    ];
ResetTokenizerCheckpoint[
  TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  ResetTokenizerCheckpoint[stream, dir];
ResetTokenizerCheckpoint[
  t_TokenStream, 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  ResetTokenizerCheckpoint[t["Stream"], dir];


WithTokenizerCheckpoint[stream_InputStream, expr_]:=
  Block[
    {
      $checkpoints = If[!AssociationQ[$checkpoints], <||>, $checkpoints],
      $peekpoints = If[!AssociationQ[$peekpoints], <||>, $peekpoints]
      },
    SetTokenizerCheckpoint[stream];
    expr
    ];
WithTokenizerCheckpoint[
  TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], 
  expr_
  ]:=
  WithTokenizerCheckpoint[stream, expr];
WithTokenizerCheckpoint[
  t_TokenStream, 
  expr_
  ]:=
  WithTokenizerCheckpoint[t["Stream"], expr];
WithTokenizerCheckpoint~SetAttributes~HoldRest;


(* ::Subsubsection::Closed:: *)
(*readStringToken*)



readStringToken[tok_, escape_:"\\"][stream_, body_, token_]:=
  Module[
    {
      tmp,
      str = Read[stream, Record, RecordSeparators->{tok}]
      },
    While[StringQ[str]&&StringEndsQ[str, escape],
      tmp = Read[stream, Record, RecordSeparators->{tok}];
      If[tmp===EndOfFile, Break[]];
      str = str<>tok<>tmp;
      ];
    str
    ]


(* ::Subsubsection::Closed:: *)
(*readLookAhead*)



readLookAhead[lookAheadDispatcher_]:=
  readLookAhead[lookAheadDispatcher, tokenPuller[Keys[lookAheadDispatcher]]];
readLookAhead[lookAheadDispatcher_, tokenPuller_][stream_, body_, token_]:=
  lookAheadDispatcher[tokenPuller[stream]][stream, body, token]


(* ::Subsubsection::Closed:: *)
(*TokenStreamer*)



(* ::Text:: *)
(*
	Designed to be as minimal overhead as can still be convenient
*)



TokenStreamer[t_TokenStream]:=
  Module[{stream=t["Stream"], spec=prepTokenHandlers@t["Tokens"]},
    TokenStreamer[stream, spec, t]
    ];
TokenStreamer[stream_, spec_, t_]:=
  TokenStreamer[{
    t,
    stream,
    spec["Handlers"], 
    spec["Characters"],
    tokenPuller[spec["Characters"]]
    }];


TokenStreamerRead[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], n_]:=
  Module[{body, tmp, token, spos},
    Table[
      token = $Failed;
      body = EndOfFile;
      While[token===$Failed,
        If[StreamPosition[stream]==0,
          (* 
                      we need a secondary handling mechanism to ensure
                        that we don't miss tokens at the very start of the stream
                      *)
          tmp = 
           Read[stream, Record,
              RecordSeparators->seps
              ];
          spos = StreamPosition[stream];
          If[StringLength@tmp<spos (* we skipped a thing *),
            SetStreamPosition[stream, 0];
            token = tokPuller[stream];
            If[ListQ@token, 
              body = token[[2]];
              token = token[[1]],
              body = "";
              ]
            ],
          (* standard mechanism a little bit simpler *)
          tmp = 
           Read[stream, Record,
              RecordSeparators->seps
              ];
          If[tmp===EndOfFile, 
            Return[
              handlers[EndOfFile][
                t,
                If[StringQ@body, body, EndOfFile],
                EndOfFile
                ],
              While
              ],
            token = tokPuller[stream];
            If[ListQ@token, 
              tmp = tmp <> token[[2]];
              token = token[[1]];
              ]
            ];
          body = If[StringQ@body, body<>tmp, tmp]
          ]
        ];
      handlers[token][
        t,
        body,
        token
        ],
      n
      ]
    ]


(
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Read"[n_]:=
  WithTokenizerCheckpoint[
    stream,
    TokenStreamerRead[tks, n]
    ];
 (
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Read"[]:=
   (tks@"Read"[1])[[1]];
 (
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Peek"[n_]:=
   WithTokenizerCheckpoint[
     stream,
     (ResetTokenizerCheckpoint[stream]; #)&@TokenStreamerRead[tks, n]
     ];
 (
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Peek"[]:=
   (tks@"Peek"[1])[[1]];


(* ::Subsubsection::Closed:: *)
(*tokenRead*)



(* ::Subsubsubsection::Closed:: *)
(*tokenPuller*)



(* ::Text:: *)
(*
	Pulls the real token off the stream. 
	Has to be a bit careful about things like \[OpenCurlyDoubleQuote]for\[CloseCurlyDoubleQuote] and EOFs.
*)



tokenPuller//Clear
tokenPuller[tokens_]:=
  tokenPuller[
    AssociationThread[tokens, None], 
    Min[StringLength/@Select[tokens, StringQ]]
    ];
tokenPuller[tokSet_, min_][stream_]:=
  pullTokenToo[stream, tokSet, min];
pullTokenToo//Clear
pullTokenToo[stream_, tokSet_, minTok_]:=
  (* gotta figure out which token we actually got... *)
  Module[{tok, tmp, spos, spos2},
    spos = StreamPosition[stream];
    If[spos>0,
      tok = Read[stream, Character];
      spos2 = StreamPosition[stream];
      If[StringMatchQ[tok, WordCharacter], 
        SetStreamPosition[stream, spos-1];
        If[!StringMatchQ[Read[stream, Character], WhitespaceCharacter],
          SetStreamPosition[stream, spos2];
          Return[{$Failed, tok}, Module],
          SetStreamPosition[stream, spos2]
          ]
        ]; 
      tok = Prepend[tok]@ReadList[stream, Character, minTok-1],
      tok = ReadList[stream, Character, minTok];
      ];
    If[AllTrue[tok, StringQ],
      tok = StringJoin[tok],
      Return[EndOfFile, Module]
      ];
    While[!KeyExistsQ[tokSet, tok],
      tmp = Read[stream, Character];
      If[tmp===EndOfFile, Return[EndOfFile, Module]];
      tok = tok<>tmp;
      ];
    
    If[StringMatchQ[StringTake[tok, {1}], WordCharacter],
       (* finally need to check that next char isn't another WordCharacter *)
      spos2 = StreamPosition[stream];
      tmp = Read[stream, Character];
      If[StringQ@tmp&&StringMatchQ[tmp, WordCharacter],
        SetStreamPosition[stream, spos];
        Return[{$Failed, tok}, Module],
        SetStreamPosition[stream, spos2]
        ]
      ];
    tok
    ]


(* ::Subsubsubsection::Closed:: *)
(*tokenRead*)



tokenRead[stream_, spec_, t_]:=
  TokenStreamer[stream, spec, t]@"Read"[];


(* ::Subsubsubsection::Closed:: *)
(*tokenPeek*)



tokenPeek[stream_, spec_, t_]:=
  TokenStreamer[stream, spec, t]@"Peek"[];


(* ::Subsubsubsection::Closed:: *)
(*tokenReadList*)



(* ::Subsubsubsubsection::Closed:: *)
(*old*)



(* ::Text:: *)
(*
	Realized this won\[CloseCurlyQuote]t work as the Handler gets applied too late...
*)



otokenReadList[stream_, spec_, n_]:=
  Module[
    {
      body, token, final, strPos,
        handlers=spec["Handlers"], 
        seps=spec["Characters"],
        nullHandle, read
        },
    body = 
      ReadList[stream, Record, n,
        RecordSeparators->seps
        ];
    body = PadRight[body, n, EndOfFile];
    final = Read[stream, Character];
    strPos = Pick[Range[n], StringQ/@body];
    If[Length@strPos < 2,
      nullHandle = Lookup[handlers, EndOfFile, LexerToken];
      Return[
        Map[
          nullHandle[
            stream,
            #,
            EndOfFile
            ]&,
          body
          ], 
        Module
        ]
      ];
    strPos = Rest@strPos;
    token = StringTake[body[[strPos]], 1];
    If[n-Max[strPos]>0,
      token = Join[token, ConstantArray[EndOfFile, n-Max[strPos]]]
      ];
    token = Append[token, final];
    MapThread[
      #[stream, #2, #3]&,
      {
        Lookup[handlers, token, LexerToken],
        body,
        token
        }
      ]
    ];


(* ::Subsubsubsubsection::Closed:: *)
(*new*)



tokenReadList[stream_, spec_, n_, t_]:=
  TokenStreamer[stream, spec, t]@"Read"[n];


(* ::Subsubsubsection::Closed:: *)
(*tokenPeekList*)



tokenPeekList[stream_, spec_, n_, t_]:=
  TokenStreamer[stream, spec, t]@"Peek"[n];


(* ::Subsubsection::Closed:: *)
(*TokenRead*)



(* ::Subsubsubsection::Closed:: *)
(*prepTokenHandlers*)



prepTokenHandler//Clear;
prepTokenHandler[{"Stream", char_, escape___}]:=readString[char, escape];
prepTokenHandler[{"LookAhead", dispatch_}]:=readLookAhead[dispatch];
prepTokenHandler[e_]:=e;


prepTokenHandlers[tokens_]:=
  ReplacePart[
    tokens,
    "Handlers"->
      Join[
        <|
          EndOfFile->LexerToken
          |>,
        Map[
          prepTokenHandler,
          tokens["Handlers"]
          ]
        ]
    ]


(* ::Subsubsubsection::Closed:: *)
(*TokenRead*)



TokenRead[t_TokenStream, n:_Integer?Positive:1]:=
  Module[{stream=t["Stream"], spec=prepTokenHandlers@t["Tokens"]},
    If[n>1, 
        tokenReadList[stream, spec, n, t],
        tokenRead[stream, spec, t]
        ]
    ];


(* ::Subsubsubsection::Closed:: *)
(*TokenRead*)



TokenPeek[t_TokenStream, n:_Integer?Positive:1]:=
  Module[{stream=t["Stream"], spec=prepTokenHandlers@t["Tokens"]},
    If[n>1, 
      tokenPeekList[stream, spec, n, t],
      tokenPeek[stream, spec, t]
      ]
    ];


End[];



