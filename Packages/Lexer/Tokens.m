(* ::Package:: *)

(* Autogenerated Package *)

(* ::Section:: *)
(*Tokens*)



TokenStream::usage="An object representing a stream of tokens";


SetTokenizerCheckpoint::usage="Sets the Tokenizer checkpoint";
ResetTokenizerCheckpoint::usage="Reverts the Tokenizer checkpoint";
WithTokenizerCheckpoint::usage="Wrapper for tokenizing";


TokenRead::usage="Pulls n tokens out of a TokenStream";
TokenStreamer::usage="Lower-level object for faster token streaming";


Begin["`Private`"];


(* ::Subsection:: *)
(*TokenStream*)



RegisterInterface[
  TokenStream,
  {
    "Tokens",
    "Stream"
    },
  "Constructor"->buildTokenStream
  ]


(* ::Subsubsubsection::Closed:: *)
(*buildTokenStream*)



buildTokenStream[l_LexerObject, i_InputStream]:=
  <|"Tokens"->l["Tokens"], "Stream"->i|>;
buildTokenStream[l_LexerObject, s_String?(Not@*FileExistsQ)]:=
  buildTokenStream[l, StringToStream[s]];
buildTokenStream[l_LexerObject, s_String?(FileExistsQ)]:=
 buildTokenStream[l, OpenRead[s]];


(* ::Subsubsubsection::Closed:: *)
(*Methods*)



InterfaceMethod[TokenStream]@
  t_TokenStream["Read"][n:_Integer?Positive:1]:=
    TokenRead[t, n];
InterfaceMethod[TokenStream]@
  t_TokenStream["Streamer"][]:=
    TokenStreamer[t];
InterfaceMethod[TokenStream]@
  t_TokenStream["Close"][]:=
    Close@t["Stream"];


(* ::Subsection:: *)
(*TokenRead*)



(* ::Subsubsection::Closed:: *)
(*$checkpoints*)



(* ::Text:: *)
(*
	Might be better to do with Language`ExpressionStore ?
	We\[CloseCurlyQuote]re gonna manage this under the assumption that if you peek ahead and then reset you want to be able to jump back to where you were post peek...?
*)



If[!AssociationQ[$checkpoints], $checkpoints=<||>];
If[!AssociationQ[$peekpoints], $peekpoints=<||>];


SetTokenizerCheckpoint[stream_InputStream, dir:"Forward"|"Backward":"Backward"]:=
  If[dir==="Backward",
    $checkpoints[stream] = StreamPosition[stream],
    $peekpoints[stream] = StreamPosition[stream]
    ];
SetTokenizerCheckpoint[
  TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  SetTokenizerCheckpoint[stream, dir];
SetTokenizerCheckpoint[
  t_TokenStream, 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  SetTokenizerCheckpoint[t["Stream"], dir];


ResetTokenizerCheckpoint[stream_InputStream, dir:"Forward"|"Backward":"Backward"]:=
  SetStreamPosition[stream, 
    If[dir==="Backward",
      $checkpoints,
      $peekpoints
      ][stream]
    ];
ResetTokenizerCheckpoint[
  TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  ResetTokenizerCheckpoint[stream, dir];
ResetTokenizerCheckpoint[
  t_TokenStream, 
  dir:"Forward"|"Backward":"Backward"
  ]:=
  ResetTokenizerCheckpoint[t["Stream"], dir];


WithTokenizerCheckpoint[stream_InputStream, expr_]:=
  Block[
    {
      $checkpoints = If[!AssociationQ[$checkpoints], <||>, $checkpoints],
      $peekpoints = If[!AssociationQ[$peekpoints], <||>, $peekpoints]
      },
    SetTokenizerCheckpoint[stream];
    expr
    ];
WithTokenizerCheckpoint[
  TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], 
  expr_
  ]:=
  WithTokenizerCheckpoint[stream, expr];
WithTokenizerCheckpoint[
  t_TokenStream, 
  expr_
  ]:=
  WithTokenizerCheckpoint[t["Stream"], expr];
WithTokenizerCheckpoint~SetAttributes~HoldRest;


(* ::Subsubsection::Closed:: *)
(*readStringToken*)



readStringToken//Clear;
readStringToken[tok_, end_, escape_:"\\"][stream_, body_, token_]:=
  Module[
    {
      sm = Replace[stream, {TokenStreamer[{_, s_, ___}]:>s, t_TokenStream:>t["Stream"]}],
      tmp,
      str,
      escs = Alternatives@@Flatten[{escape}]
      },
    str = Read[sm, Record, RecordSeparators->{end}, NullRecords->True];
    While[StringQ[str]&&StringEndsQ[str, escs],
      tmp = Read[sm, Record, RecordSeparators->{end}, NullRecords->True];
      If[tmp===EndOfFile, Break[]];
      str = str<>tok<>tmp;
      ];
    LexerToken[stream, {body, tok<>str<>end}, tok]
    ]


(* ::Subsubsection::Closed:: *)
(*readLookAhead*)



readLookAhead[lookAheadDispatcher_]:=
  readLookAhead[lookAheadDispatcher, tokenPuller[Keys[lookAheadDispatcher]]];
readLookAhead[lookAheadDispatcher_, tokenPuller_][stream_, body_, token_]:=
  lookAheadDispatcher[tokenPuller[stream]][stream, body, token]


(* ::Subsubsection::Closed:: *)
(*TokenStreamer*)



(* ::Text:: *)
(*
	Designed to be as minimal overhead as can still be convenient
*)



(* ::Subsubsubsection::Closed:: *)
(*TokenStreamer*)



TokenStreamer[t_TokenStream]:=
  Module[{stream=t["Stream"], spec=prepTokenHandlers@t["Tokens"]},
    TokenStreamer[stream, spec, t]
    ];
TokenStreamer[stream_, spec_, t_]:=
  TokenStreamer[{
    t,
    stream,
    spec["Handlers"], 
    spec["Characters"],
    tokenPuller[spec["Characters"]]
    }];


(
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Read"[n_]:=
  WithTokenizerCheckpoint[
    stream,
    TokenStreamerRead[tks, n]
    ];
 (
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Read"[]:=
   (tks@"Read"[1])[[1]];
 (
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Peek"[n_]:=
   WithTokenizerCheckpoint[
     stream,
     (ResetTokenizerCheckpoint[stream]; #)&@TokenStreamerRead[tks, n]
     ];
 (
  tks:HoldPattern[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}]]
  )@"Peek"[]:=
   (tks@"Peek"[1])[[1]];


(* ::Subsubsubsection::Closed:: *)
(*readRecord*)



If[!MatchQ[$unfortunatelyNecessary, _Language`ExpressionStore],
  $unfortunatelyNecessary=Language`NewExpressionStore["<StreamBugBits>"];
  ];


alreadyReadOnce[stream_, seps_]:=
  TrueQ[$unfortunatelyNecessary@"get"[stream, seps]]


readRecordReal[stream_, seps_]:=
  Read[stream, 
    Record,
    RecordSeparators->seps,
    NullRecords->True
    ];
readRecord[stream_, seps_, sp_]:=
  (* 
        this works around a bug in InputStream where the stream 
          can advance internally, but the StreamPosition doesn't 
        *)
  If[sp==0,
    If[alreadyReadOnce[stream, seps],
      SetStreamPosition[stream, 0];
      readRecordReal[stream, seps];
      readRecordReal[stream, seps],
      $unfortunatelyNecessary@"put"[stream, seps, True];
      readRecordReal[stream, seps]
      ],
    readRecordReal[stream, seps]
    ];


(* ::Subsubsubsection::Closed:: *)
(*TokenStreamerRead*)



TokenStreamerRead[TokenStreamer[{t_, stream_, handlers_, seps_, tokPuller_}], n_]:=
  Module[{body, tmp, token, spos, sp},
    Table[
      token = $Failed;
      body = EndOfFile;
      Block[{}, (* just building a Return point *)
        sp = StreamPosition[stream];
        While[token===$Failed,
          If[sp==0,
            (* 
                            we need a secondary handling mechanism to ensure
                              that we don't miss tokens at the very start of the stream
                            *)
            tmp = 
             readRecord[stream, seps, sp];
            (* if we're at the end of the stream just bail *)
            If[tmp===EndOfFile,
              Return[
                handlers[EndOfFile][
                  t,
                  EndOfFile,
                  EndOfFile
                  ],
                Block
                ]
              ];
            spos = StreamPosition[stream];
            If[StringLength@tmp<spos&&!alreadyReadOnce[stream, seps](* we skipped too far *),
              SetStreamPosition[stream, 0];
              token = tokPuller[stream];
              If[ListQ@token, 
                body = token[[2]];
                token = token[[1]],
                body = "";
                ],
              token = tokPuller[stream];
              If[ListQ@token, 
                tmp = tmp <> token[[2]];
                token = token[[1]];
                ];
              body = If[StringQ@body, body<>tmp, tmp]
              ],
            (* standard mechanism a little bit simpler *)
            tmp = readRecord[stream, seps, sp];
            (* if we're at the end of the stream, again, just bail *)
            If[tmp===EndOfFile, 
              Return[
                handlers[EndOfFile][
                  t,
                  If[StringQ@body, body, EndOfFile],
                  EndOfFile
                  ],
                Block
                ],
              token = tokPuller[stream];
              If[ListQ@token, 
                tmp = tmp <> token[[2]];
                token = token[[1]];
                ]
              ];
            body = If[StringQ@body, body<>tmp, tmp]
            ]
          ];
        handlers[token][
          t,
          body,
          token
          ]
        ],
      n
      ]
    ]


(* ::Subsubsection::Closed:: *)
(*tokenRead*)



(* ::Subsubsubsection::Closed:: *)
(*tokenTrie*)



(* ::Text:: *)
(*
	Need some little trie functionality to optimize this token pulling
*)



addDefTok[assoc_, cur_:""]:=
  AssociationMap[
    With[{cur1=cur<>#[[1]]},
      #[[1]]->
        Map[
          If[AssociationQ[#], addDefTok[#, cur1], #]&,
          If[AssociationQ@#[[2]], Append[#[[2]], Default->cur1], #[[2]]]
          ]
      ]&,
    assoc
    ];
groupToks[strings_, pos_]:=
  With[{longer=Select[strings, StringLength@#>=pos&]},
    Which[
      Length@strings==1,
        strings[[1]],
      Length@longer>0,
        GroupBy[longer, StringTake[#, {pos}]&, groupToks[#, pos+1]&],
      True,
        strings[[1]]
      ]
    ];
tokenTrie[strings_]:=
  addDefTok@groupToks[strings, 1]


(* ::Subsubsubsection::Closed:: *)
(*getTokenViaTrie*)



getTokenViaTrie[stream_, trie_]:=
  (* keep walking through the trie until we find what token was returned *)
  Module[{c, t = trie, t2, i=1, sp = StreamPosition[stream]},
    While[AssociationQ@t,
      c = Read[stream, Character];
      If[c===EndOfFile, t=EndOfFile;Break[]];
      t2 = t[c];
      If[StringQ@t2&&StringLength[t2]>i, 
        Skip[stream, Character, StringLength[t2]-i]
        ];
      If[MissingQ@t2,  
        t2 = t[Default];
        SetStreamPosition[stream, sp+(i-1)]
        ];
      t = t2;
      i++
      ];
    t
    ]


(* ::Subsubsubsection::Closed:: *)
(*tokenPuller*)



(* ::Text:: *)
(*
	Pulls the real token off the stream. 
	Has to be a bit careful about things like \[OpenCurlyDoubleQuote]for\[CloseCurlyDoubleQuote] and EOFs.
*)



tokenPuller//Clear
tokenPuller[tokens_]:=
  tokenPuller[
    tokenTrie[Select[tokens, StringQ]], 
    Min[StringLength/@Select[tokens, StringQ]]
    ];
tokenPuller[tokSet_, min_][stream_]:=
  pullTokenToo[stream, tokSet, min];
pullTokenToo//Clear
pullTokenToo[stream_, tokTrie_, minTok_]:=
  (* gotta figure out which token we actually got... *)
  Module[{tok, tmp, spos, spos2},
    spos = StreamPosition[stream];
    If[spos>0,
      (* ensure that if we're in a word-type token a non-word preceded it *)
      tok = Read[stream, Character];
      spos2 = StreamPosition[stream];
      If[StringQ@tok&&StringMatchQ[tok, WordCharacter], 
        SetStreamPosition[stream, spos-1];
        If[StringMatchQ[Read[stream, Character], WordCharacter],
          SetStreamPosition[stream, spos2];
          Return[{$Failed, tok}, Module]
          ]
        ];
      SetStreamPosition[stream, spos];
      ];
    tok = getTokenViaTrie[stream, tokTrie];
    If[tok === EndOfFile, Return[tok, Module]];
    If[StringMatchQ[StringTake[tok, {1}], WordCharacter],
       (* finally need to check that next char isn't another WordCharacter *)
      tmp = Read[stream, Character];
      If[StringQ@tmp&&StringMatchQ[tmp, WordCharacter],
        SetStreamPosition[stream, spos];
        Return[{$Failed, tok}, Module]
        ]
      ];
    SetStreamPosition[stream, spos];
    (*Which[(* get stream ready for next Read call *)
      StringQ[tok]&&StringLength[tok]>1, 
        SetStreamPosition[stream, StreamPosition[stream]-1],
      spos>0,
        SetStreamPosition[stream, spos]
      ]*);
    tok
    ]


(* ::Subsubsubsection::Closed:: *)
(*tokenRead*)



tokenRead[stream_, spec_, t_]:=
  TokenStreamer[stream, spec, t]@"Read"[];


(* ::Subsubsubsection::Closed:: *)
(*tokenPeek*)



tokenPeek[stream_, spec_, t_]:=
  TokenStreamer[stream, spec, t]@"Peek"[];


(* ::Subsubsubsection::Closed:: *)
(*tokenReadList*)



(* ::Subsubsubsubsection::Closed:: *)
(*new*)



tokenReadList[stream_, spec_, n_, t_]:=
  TokenStreamer[stream, spec, t]@"Read"[n];


(* ::Subsubsubsection::Closed:: *)
(*tokenPeekList*)



tokenPeekList[stream_, spec_, n_, t_]:=
  TokenStreamer[stream, spec, t]@"Peek"[n];


(* ::Subsubsection::Closed:: *)
(*TokenRead*)



(* ::Subsubsubsection::Closed:: *)
(*prepTokenHandlers*)



prepTokenHandler//Clear;
prepTokenHandler[{"Stream", char_, escape___}]:=readString[char, escape];
prepTokenHandler[{"LookAhead", dispatch_}]:=readLookAhead[dispatch];
prepTokenHandler[e_]:=e;


prepTokenHandlers[tokens_]:=
  ReplacePart[
    tokens,
    "Handlers"->
      Join[
        <|
          EndOfFile->LexerToken
          |>,
        Map[
          prepTokenHandler,
          tokens["Handlers"]
          ]
        ]
    ]


(* ::Subsubsubsection::Closed:: *)
(*TokenRead*)



TokenRead[t_TokenStream, n:_Integer?Positive:1]:=
  Module[{stream=t["Stream"], spec=prepTokenHandlers@t["Tokens"]},
    If[n>1, 
        tokenReadList[stream, spec, n, t],
        tokenRead[stream, spec, t]
        ]
    ];


(* ::Subsubsubsection::Closed:: *)
(*TokenRead*)



TokenPeek[t_TokenStream, n:_Integer?Positive:1]:=
  Module[{stream=t["Stream"], spec=prepTokenHandlers@t["Tokens"]},
    If[n>1, 
      tokenPeekList[stream, spec, n, t],
      tokenPeek[stream, spec, t]
      ]
    ];


End[];



